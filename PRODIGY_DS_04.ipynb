{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing necessary libraries:**"
      ],
      "metadata": {
        "id": "xYYZ-Cbjzjsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, BatchNormalization\n",
        "import pandas\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.callbacks import LearningRateScheduler\n",
        "import numpy as np\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from google.colab import drive\n",
        "from tensorflow.keras import regularizers"
      ],
      "metadata": {
        "id": "gjeHFx2Ht9DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Natural Language Toolkit:**"
      ],
      "metadata": {
        "id": "jG6QGWW1zgk8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7XF-o_F1t9F9",
        "outputId": "b3298780-b2fa-4a69-a64a-adf6ef4ba98a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_reddit = pandas.read_csv(\"Reddit.csv\")\n",
        "data_twitter = pandas.read_csv(\"Twitter.csv\")\n",
        "data_reddit.rename(columns = {'clean_comment': 'text'}, inplace = True)\n",
        "data_twitter.rename(columns = {'clean_text': 'text'}, inplace = True)\n",
        "\n",
        "data = pandas.concat([data_reddit, data_twitter], ignore_index = True)\n",
        "data = pandas.DataFrame(data)\n",
        "data.dropna(axis = 0, inplace = True)"
      ],
      "metadata": {
        "id": "oEyHc-L6zaUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing Text**"
      ],
      "metadata": {
        "id": "QAl2jE1LxQTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    if isinstance(text, str):\n",
        "\n",
        "        text = text.lower()\n",
        "\n",
        "        tokens = word_tokenize(text)\n",
        "        tokens = [token for token in tokens if token.isalnum()]\n",
        "\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "        preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "        return preprocessed_text\n",
        "    else:\n",
        "        return \"\"\n",
        "\n",
        "def tokenize(item):\n",
        "  tokenizer = Tokenizer(oov_token=\"\")\n",
        "  tokenizer.fit_on_texts(item)\n",
        "  sequences = tokenizer.texts_to_sequences(item)\n",
        "  max_length = 65\n",
        "  padded_sequences = pad_sequences(sequences, maxlen=max_length, padding='post', truncating='post')\n",
        "  vocab_size = len(tokenizer.word_index) + 1\n",
        "  return padded_sequences, vocab_size\n",
        "\n",
        "def lr_schedule(epoch, lr):\n",
        "    return lr * np.exp(1)"
      ],
      "metadata": {
        "id": "SLhcFvW8t9Ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ordinal_mapping = {-1: 0, 0: 1, 1: 2}\n",
        "\n",
        "\n",
        "data['encoded_sentiment'] = data['category'].apply(lambda x: ordinal_mapping[x])\n",
        "data['preprocessed_text'] = data['text'].apply(preprocess_text)"
      ],
      "metadata": {
        "id": "_YrKUQPLt9LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Splitting training and test data:**"
      ],
      "metadata": {
        "id": "1va3d1VJ-IfC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = data.pop(\"preprocessed_text\")\n",
        "y = data.pop('encoded_sentiment')\n",
        "x, vocab_size = tokenize(x)\n",
        "y = to_categorical(y, 3)\n",
        "\n",
        "X_train,X_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=42)\n",
        "\n",
        "X_train = tf.convert_to_tensor(X_train)\n",
        "y_train = tf.convert_to_tensor(y_train)\n",
        "X_test = tf.convert_to_tensor(X_test)\n",
        "y_test = tf.convert_to_tensor(y_test)"
      ],
      "metadata": {
        "id": "Qxn1tRP7t9d7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.6, patience=0, verbose=1, min_lr=1e-6)\n",
        "\n",
        "embedding_dim = 65\n",
        "max_length = 65"
      ],
      "metadata": {
        "id": "hZfPO1kNz0AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training the model using LSTM:**"
      ],
      "metadata": {
        "id": "4zkTmxwo-TZj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length))\n",
        "model.add(LSTM(8, return_sequences=True))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(rate=0.4))\n",
        "model.add(LSTM(8, return_sequences=True))\n",
        "model.add(Dropout(rate=0.4))\n",
        "model.add(LSTM(8, return_sequences=True))\n",
        "model.add(Dropout(rate=0.2))\n",
        "model.add(LSTM(8))\n",
        "model.add(Dropout(rate=0.2))\n",
        "model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=tf.keras.optimizers.Adam(learning_rate=0.004), metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=25, validation_split=0.2, callbacks=[early_stopping,lr_scheduler])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clyCnA-80DgH",
        "outputId": "83c55a23-661b-4085-bb60-2fe4914f383e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "4253/4253 [==============================] - 130s 28ms/step - loss: 0.4971 - accuracy: 0.8304 - val_loss: 0.3699 - val_accuracy: 0.8869 - lr: 0.0040\n",
            "Epoch 2/25\n",
            "4253/4253 [==============================] - ETA: 0s - loss: 0.3680 - accuracy: 0.8947\n",
            "Epoch 2: ReduceLROnPlateau reducing learning rate to 0.002400000113993883.\n",
            "4253/4253 [==============================] - 82s 19ms/step - loss: 0.3680 - accuracy: 0.8947 - val_loss: 0.3789 - val_accuracy: 0.8907 - lr: 0.0040\n",
            "Epoch 3/25\n",
            "4253/4253 [==============================] - 86s 20ms/step - loss: 0.3085 - accuracy: 0.9167 - val_loss: 0.3111 - val_accuracy: 0.9100 - lr: 0.0024\n",
            "Epoch 4/25\n",
            "4253/4253 [==============================] - ETA: 0s - loss: 0.2658 - accuracy: 0.9283\n",
            "Epoch 4: ReduceLROnPlateau reducing learning rate to 0.00144000006839633.\n",
            "4253/4253 [==============================] - 79s 18ms/step - loss: 0.2658 - accuracy: 0.9283 - val_loss: 0.3251 - val_accuracy: 0.9061 - lr: 0.0024\n",
            "Epoch 5/25\n",
            "4253/4253 [==============================] - ETA: 0s - loss: 0.2188 - accuracy: 0.9420\n",
            "Epoch 5: ReduceLROnPlateau reducing learning rate to 0.0008640000130981206.\n",
            "4253/4253 [==============================] - 83s 19ms/step - loss: 0.2188 - accuracy: 0.9420 - val_loss: 0.3373 - val_accuracy: 0.9087 - lr: 0.0014\n",
            "Epoch 6/25\n",
            "4253/4253 [==============================] - ETA: 0s - loss: 0.1804 - accuracy: 0.9536\n",
            "Epoch 6: ReduceLROnPlateau reducing learning rate to 0.0005184000008739531.\n",
            "4253/4253 [==============================] - 81s 19ms/step - loss: 0.1804 - accuracy: 0.9536 - val_loss: 0.3566 - val_accuracy: 0.9060 - lr: 8.6400e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing the model:**"
      ],
      "metadata": {
        "id": "G19Z_Q3k-YOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test,y_test)"
      ],
      "metadata": {
        "id": "SiFugv7g0NBu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab772c5c-2380-4c85-bde3-c7fe09d8d93a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "939/939 [==============================] - 6s 6ms/step - loss: 0.3199 - accuracy: 0.9082\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3199221193790436, 0.9082217216491699]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OBJYTjtd4riq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}